\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
This thesis explores the impact of dynamic entropy tuning in Reinforcement Learning (RL) algorithms that train a stochastic policy against the algorithms that train a deterministic one. Both algorithms are widely used for designing quadcopter low-level controllers. Stochastic policies optimize a probability distribution over actions to maximize rewards, while deterministic policies select a single deterministic action per state. The effect of training a stochastic policy and then executing deterministic actions in the environment is compared to having both deterministic training and actions for stabilizing the quadcopter. For this research, the Soft Actor-Critic (SAC) algorithm was chosen for the stochastic algorithm while the Twin Delayed Deep Deterministic Policy Gradient (TD3) was chosen for the deterministic algorithm. Through training and simulation results, the effect of dynamic entropy on stabilizing the quadcopter in the stochastic policy is compared against the deterministic policy in different state and action spaces. The results show the positive effect the dynamic entropy tuning has on stochastic policies in controlling and stabilizing the quadcopter. Next, the stochastic algorithm is used to develop a position controller that can effectively navigate the quadcopter and track any given trajectory. The controller was then tested against external disturbances in the form of wind disturbances to test its robustness. The simulation results showed the effectiveness and robustness of the proposed controller. Finally, the controllerâ€™s compatibility with different quadcopters containing an attitude controller was tested and proven through simulation results.
\clearpage