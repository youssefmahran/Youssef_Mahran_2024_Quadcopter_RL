\chapter{Introduction}
    \section{Motivation}
    \subsection{UAVs and Multicopters}
    Unmanned Aerial Vehicles (UAVs) have become revolutionary technologies that are changing many industries and having a significant influence on modern society. The uses for UAVs are endless, ranging from military surveillance and applications to civilian applications like aerial photography, first-aid delivery, and disaster assistance. Furthermore, the quick development of UAV technology has made aerial capabilities more accessible to a wider range of people, enabling companies, organizations, and individuals to take advantage of unmanned flights.\\
    
    Multicopters, such as quadcopters, have become highly popular due to their flexibility and ability to carry out complex aerial tasks. They are especially well-suited for applications that need vertical movements and stationary hovering, such as aerial photography, cinematography, and surveying. Advanced quadcopter technology has become increasingly accessible due to to its relatively simple design and low cost, enabling professionals, researchers, and enthusiasts to all benefit from unmanned aerial operations. The impact of UAVs on modern society is expected to increase as quadcopter technology develops with more breakthroughs expected in a variety of sectors.
    %############################################################
    \subsection{Advanced Control Algorithms}
    Robust control algorithms are essential for increasing quadcopter performance and improving their stability due to their unstable nature. Proportional-Integral-Derivative (PID) controllers are a particularly good method for stabilizing quadcopter because they are easy to use and work well by adjusting control inputs in response to error signals. PID controllers are a popular choice for basic control jobs because of their reliable performance under a range of conditions. On the other hand, Model Predictive Control (MPC) and Sliding Mode Control (SMC) provide practical solutions for complex scenarios in need of adaptive and nonlinear control. A predictive model is used by MPC to predict future system behavior and calculate the optimal course of control, whereas SMC uses robust sliding surfaces to maintain tracking performance and stability regardless of uncertainties and disturbances. Although many quadcopter control problems can be effectively solved with these advanced control techniques, the addition of Reinforcement Learning (RL) introduces an entirely new approach by allowing autonomous agents to interact with their environment and learn the best control strategies.
    %###########################################################
    \subsection{Reinforcement Learning}
    RL stands out as a groundbreaking approach in artificial intelligence, particularly for its role as a control platform in autonomous systems. Fundamentally, RL offers a framework through which agents can interact with their surroundings to learn optimal methods for taking action. In contrast to conventional control techniques, which frequently rely on pre-programmed algorithms or models, RL enables agents to learn and adapt to their environment and modify their behavior accordingly. In tasks involving uncertainty and unpredictable circumstances, RL is especially ideal since it provides the agent with feedback, usually in the form of rewards or penalties, to help it achieve its goals. Due to its versatility, RL has been widely adopted in a variety of fields, including industrial control systems, robotics, and autonomous vehicles. Through the use of RL as a control platform, researchers and industry professionals have opened up new possibilities for developing intelligent and adaptive systems that can do complex tasks autonomously.
    %###########################################################
    \subsection{Integrating Deep RL Control with Quadcopters}
    RL presents an exciting approach to handling the complexities in the quadcopter design and offers a promising path towards enhancing its control. Quadcopters can use RL algorithms to autonomously learn the best control methods through interacting with their surrounding environment. With the help of this adaptive learning process, quadcopters can adjust their behavior in response to feedback they receive, allowing for accurate maneuvering and effective task completion in a wide range of situations. Quadcopters can autonomously navigate dynamic and unexpected surroundings, optimize trajectories, and limit hazards thanks to RL-based control frameworks. Recent advancements in RL allowed quadcopters to obtain champion-level drone racing performance \cite{nature}. Using a live video feed from the quadcopter's camera, the quadcopter was able to navigate the race course and pass through all the gates in correct sequence faster than the human-controlled quadcopter. The propsed algorithm was able to win against 3 different drone racing champions and achieved the fastest recorded race time. This shows the potential of using RL control framework in controlling quadcopters.
    %#############################################################
    \section{Thesis Aim}
    This thesis aims to explore the effect of dynamic entropy tuning on quadcopter low-level control. With the deterministic algorithms being widely used in research as an RL algorithm in controlling quadcopters, this thesis aims to compare stochastic and deterministic policy training. Evaluation of the two policies is done in a deterministic environment to determine the optimal policy training method. The comparison between different policy training techniques and the effect of dynamic entropy tuning on controlling quadcopters was never proposed in the literature. The thesis then aims to use the stochastic algorithm in developing a position controller that can stabilize the drone at a certain position and track any given trajectory. This is done by calculating the desired thrust and attitude and sending this data to an attitude controller. This framework was never presented in the literature before. 
    %############################################################
    \section{Thesis Outline}
    This thesis shows the research done to achieve the thesis aim. The rest of the thesis is organized as follows:
    \begin{itemize}
        \item \textbf{Chapter 2} gives a brief introduction to the concepts of Machine Learning, RL, Neural Networks, and Deep RL. It also introduces the algorithms used in this thesis, TD3 and SAC algorithms, along with a summary of them. Previous research work is also presented.
        \item \textbf{Chapter 3} presents the implementation of the training environment, the structure of the used agents and all the steps that was required in this research.
        \item \textbf{Chapter 4} shows the outcome of the deterministic and stochastic algorithms and compares their results against each other along with a discussion about the different behavior. Results of controlling and stabilizing the qudcopter are then shown with a robustness test for the proposed position controller.
        \item \textbf{Chapter 5} concludes the thesis and presents possible future work.
    \end{itemize}

\clearpage