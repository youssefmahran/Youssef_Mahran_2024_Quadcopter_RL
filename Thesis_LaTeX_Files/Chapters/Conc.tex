\chapter{Conclusion \& Future Work}
\section{Conclusion}
   In this thesis, the effect of dynamic entropy tuning in RL algorithms on controlling and stabilizing the quadcopter was presented. The effects of having stochastic training in contrast to deterministic training and then controlling the quadcopter in a deterministic environment were also explored. The TD3 and SAC algorithms, representing a deterministic algorithm and a stochastic algorithm with dynamic entropy tuning respectively, were used to design two different low-level quadcopter controllers for each agent. Training results showcased a faster training time and higher rewards for the stochastic algorithm in contrast to the deterministic one. Simulation results showcased the effect of the stochasticity and entropy tuning in helping the SAC algorithm learn the entirety of the state space faster than the deterministic TD3 algorithm. Not only that, but the randomness introduced by the entropy term allowed the SAC algorithm to successfully navigate the quadcopter in states beyond the trained state space. A SAC-based position controller was also proposed and trained to stabilize at a certain position and track any given trajectory. The simulation results showed the controller's efficiency and ability to execute the required tasks. Wind disturbances were then added to test the robustness of the controller in minimizing the disturbances. Finally, the agent's compatibility with different quadcopters was tested. Simulation results showed the robustness and effectiveness of the proposed controller in controlling and navigating the quadcopter.\clearpage
   
   \section{Future Work}
   Future work includes integrating the proposed controller with Ardupilot's attitude controller using Ardupilot Software-in-the-Loop (SITL) simulation through ROS and simulating the quadcopter in Gazebo-ROS. Practical experiments can be carried out after successful integration with Ardupilot with flying the quadcopter in a practical setup. Model-Based RL techniques can be explored to accelerate the learning process through using a model of the environment to predict the next states and maximize the potential rewards.
\clearpage