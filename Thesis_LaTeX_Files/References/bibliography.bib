@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}
 @Manual{blender,
   title = {Blender - a 3D modelling and rendering package},
   author = {Blender Online Community},
   organization = {Blender Foundation},
   address = {Stichting Blender Foundation, Amsterdam},
   year = {2018},
   url = {http://www.blender.org},
 } 
@INPROCEEDINGS{ros,
author={Morgan Quigley and Brian Gerkey and Ken Conley and Josh Faust and
Tully Foote and Jeremy Leibs and Eric Berger and Rob Wheeler and Andrew Ng},
title={ROS: an open-source Robot Operating System},
booktitle={roc. of the IEEE Intl. Conf. on Robotics and Automation (ICRA)
Workshop on Open Source Robotics},
month = {May},
year={2009},
address={Kobe, Japan}
}
@misc{arducopter,
    author       = "{ArduPilot}",
    title        = "{ArduCopter: ArduPilot Copter UAV implementation}",
    howpublished = "\url{https://ardupilot.org/copter/}",
    year         = {2023}
}
@INPROCEEDINGS{Gazebo,

  author={Koenig, N. and Howard, A.},

  booktitle={2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)}, 

  title={Design and use paradigms for Gazebo, an open-source multi-robot simulator}, 

  year={2004},

  volume={3},

  number={},

  pages={2149-2154 vol.3},

  keywords={Open source software;Educational robots;Vehicle dynamics;Robot sensing systems;Packaging;Computational modeling;Service robots;Mobile robots;Friction;Testing},

  doi={10.1109/IROS.2004.1389727}}

@misc{solidworks,
    author       = "{Dassault Systèmes}",
    title        = "{Solidworks CAD Software}",
    howpublished = "\url{https://www.solidworks.com/}",
    year         = {2023}
}
@inproceedings{pytorch,
    author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
    title = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
    year = {2024},
    isbn = {9798400703850},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3620665.3640366},
    doi = {10.1145/3620665.3640366},
    abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
    booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
    pages = {929–947},
    numpages = {19},
    location = {, La Jolla, CA, USA, },
    series = {ASPLOS '24}
    }

      


@INPROCEEDINGS{pybullet,
      title={Learning to Fly---a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control}, 
      author={Jacopo Panerati and Hehui Zheng and SiQi Zhou and James Xu and Amanda Prorok and Angela P. Schoellig},
      booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
      year={2021},
      volume={},
      number={},
      pages={7512-7519},
      doi={10.1109/IROS51168.2021.9635857}
}
@ARTICLE{mavlink,
  author={Koubâa, Anis and Allouch, Azza and Alajlan, Maram and Javed, Yasir and Belghith, Abdelfettah and Khalgui, Mohamed},
  journal={IEEE Access}, 
  title={Micro Air Vehicle Link (MAVlink) in a Nutshell: A Survey}, 
  year={2019},
  volume={7},
  number={},
  pages={87658-87680},
  keywords={Protocols;Tutorials;Drones;Reliability;Payloads;Robots;MAVLink;ArduPilot;PX4;Unmanned Aerial Vehicles (UAVs);Ground Control Stations (GCSs)},
  doi={10.1109/ACCESS.2019.2924410}}
@book{ubutnu,
  title={A practical guide to Ubuntu Linux},
  author={Sobell, Mark G},
  year={2015},
  publisher={Pearson Education}
}

@inbook{first,
title = "A Reinforcement Learning Approach for Autonomous Control and Landing of a Quadrotor",
abstract = "— This paper looks into the problem of precise autonomous landing of an Unmanned Aerial Vehicle (UAV) which is considered to be a difficult problem as one has to generate appropriate landing trajectories in presence of dynamic constraints, such as, sudden changes in wind velocities and directions, downwash effects, change in payload etc. The problem is further compounded due to uncertainties arising from inaccurate model information and noisy sensor readings. The problem is partially solved by proposing a Reinforcement Learning (RL) based controller that uses Least Square Policy Iteration (LSPI) to learn the optimal control policies required for generating these trajectories. The efficacy of the approach is demonstrated through both simulation and real-world ex-periments with actual Parrot AR drone 2.0. According to our study, this is the first time such experimental results have been presented using RL based controller for drone landing, making it a novel contribution in this field.",
author = "Vankadari, {Madhu Babu} and Kaushik Das and Chinmay Shinde and Swagat Kumar",
year = "2018",
month = aug,
day = "31",
doi = "10.1109/ICUAS.2018.8453468",
language = "English",
isbn = "9781538613535",
series = "2018 International Conference on Unmanned Aircraft Systems, ICUAS 2018",
publisher = "Institute of Electrical and Electronics Engineers Inc.",
pages = "676--683",
booktitle = "2018 International Conference on Unmanned Aircraft Systems, ICUAS 2018",
address = "United States",
note = "2018 International Conference on Unmanned Aircraft Systems, ICUAS 2018 ; Conference date: 12-06-2018 Through 15-06-2018",
}

@ARTICLE{second,
  author={Lin, Xiaobo and Yu, Yao and Sun, Changyin},
  journal={IEEE Access}, 
  title={Supplementary Reinforcement Learning Controller Designed for Quadrotor UAVs}, 
  year={2019},
  volume={7},
  number={},
  pages={26422-26431},
  keywords={Training;Performance analysis;Reinforcement learning;Nonlinear dynamical systems;Uncertainty;Convergence;Mathematical model;Quadrotor;UAVs;reinforcement learning;ADP;control system},
  doi={10.1109/ACCESS.2019.2901295}}

@INPROCEEDINGS{Mokhtar,
  author={Mokhtar, Mohamed and El-Badawy, Ayman},
  booktitle={2023 International Conference on Unmanned Aircraft Systems (ICUAS)}, 
  title={Autonomous Navigation and Control of a Quadrotor Using Deep Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1045-1052},
  keywords={Training;Deep learning;Reinforcement learning;Aerospace electronics;Behavioral sciences;Space exploration;Collision avoidance},
  doi={10.1109/ICUAS57906.2023.10156126}}

@INPROCEEDINGS{mazen,
  author={Shehab, Mazen and Zaghloul, Ahmed and El-Badawy, Ayman},
  booktitle={2021 18th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE)}, 
  title={Low-Level Control of a Quadrotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  keywords={Training;Adaptation models;Machine learning algorithms;Uncertainty;Trajectory tracking;Computational modeling;Neural networks;Quadrotor;Machine Learning;Reinforcement Learning;TD3},
  doi={10.1109/CCE53527.2021.9633086}}

@article{
flightmare,
author = {Antonio Loquercio  and Elia Kaufmann  and René Ranftl  and Matthias Müller  and Vladlen Koltun  and Davide Scaramuzza },
title = {Learning high-speed flight in the wild},
journal = {Science Robotics},
volume = {6},
number = {59},
pages = {eabg5810},
year = {2021},
doi = {10.1126/scirobotics.abg5810},
URL = {https://www.science.org/doi/abs/10.1126/scirobotics.abg5810},
}

@article{Airsim,
  author       = {Shital Shah and
                  Debadeepta Dey and
                  Chris Lovett and
                  Ashish Kapoor},
  title        = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous
                  Vehicles},
  journal      = {CoRR},
  volume       = {abs/1705.05065},
  year         = {2017},
  url          = {http://arxiv.org/abs/1705.05065},
  eprinttype    = {arXiv},
  eprint       = {1705.05065},
  timestamp    = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ShahDLK17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{crazys,

  author={Silano, Giuseppe and Aucone, Emanuele and Iannelli, Luigi},

  booktitle={2018 26th Mediterranean Conference on Control and Automation (MED)}, 

  title={CrazyS: A Software-In-The-Loop Platform for the Crazyflie 2.0 Nano-Quadcopter}, 

  year={2018},

  volume={},

  number={},

  pages={1-6},

  keywords={Mathematical model;Atmospheric modeling;Aircraft;Rotors;Drones;Robot sensing systems;Vehicle dynamics},

  doi={10.1109/MED.2018.8442759}}


@article{HIMANSHU2022281,
title = {Waypoint Navigation of Quadrotor using Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {22},
pages = {281-286},
year = {2022},
note = {22nd IFAC Symposium on Automatic Control in Aerospace ACA 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.03.047},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323003051},
author = {K. Himanshu and Hari kumar and Jinraj V Pushpangathan},
keywords = {Reinforcement learning control, Tracking, Navigation, UAVs, Intelligent robotics},
abstract = {This paper proposes a Reinforcement Learning (RL) based technique to develop a simple neural network controller for the task of waypoint navigation in quadrotors. In this paper, the application of Twin Delayed Deep Deterministic (TD3) Policy Gradient algorithm for high and low-level control implementation for quadrotors is discussed. The proposed methods are tested on high fidelity Gym-Pybullet-Drones simulator. The effectiveness of the methods developed is validated through numerical simulations. The simulation results indicate that both control policies are successful in navigating through the assigned waypoint, with the low-level controller being accurate in the nominal flight conditions. In the presence of disturbance inputs, the high-level controller performs better when compared to the low-level controller.}
}

@article{refId0,
	author = {{Hohbach, Annika} and {Jordaan, Hendrik Willem} and {Engelbrecht, Japie}},
	title = {Model-Free trajectory planning for a rotary-wing unmanned aerial vehicle with an uncertain suspended payload},
	DOI= "10.1051/matecconf/202338804019",
	url= "https://doi.org/10.1051/matecconf/202338804019",
	journal = {MATEC Web Conf.},
	year = 2023,
	volume = 388,
	pages = "04019",
}

@article{DBLP:journals/corr/abs-2010-02293,
  author       = {Gabriel Moraes Barros and
                  Esther Luna Colombini},
  title        = {Using Soft Actor-Critic for Low-Level {UAV} Control},
  journal      = {CoRR},
  volume       = {abs/2010.02293},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.02293},
  eprinttype    = {arXiv},
  eprint       = {2010.02293},
  timestamp    = {Mon, 12 Oct 2020 17:53:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-02293.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2109-10488,
  author       = {Paras Sharma and
                  Prithvi Poddar and
                  P. B. Sujit},
  title        = {A Model-free Deep Reinforcement Learning Approach To Maneuver {A}
                  Quadrotor Despite Single Rotor Failure},
  journal      = {CoRR},
  volume       = {abs/2109.10488},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.10488},
  eprinttype    = {arXiv},
  eprint       = {2109.10488},
  timestamp    = {Mon, 27 Sep 2021 15:21:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-10488.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{Raj_2022, title={Supervised, Unsupervised and Semi-supervised Learning with Real-life Usecases}, url={https://www.enjoyalgorithms.com/blogs/supervised-unsupervised-and-semisupervised-learning}, journal={Enjoy Algorithms}, author={Raj, Ravish}, year={2022}} 

@misc{RL,
author = {Rabault, J. and Ren, Feng and Zhang, Wei and Tang, Hui and Xu, Hui},
year = {2020},
month = {01},
pages = {},
title = {DEEP REINFORCEMENT LEARNING IN FLUID MECHANICS: A PROMISING METHOD FOR BOTH ACTIVE FLOW CONTROL AND SHAPE OPTIMIZATION},
doi = {10.13140/RG.2.2.29692.28800}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@misc{lillicrap2019continuous,
      title={Continuous control with deep reinforcement learning}, 
      author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
      year={2019},
      eprint={1509.02971},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{haarnoja2019soft,
      title={Soft Actor-Critic Algorithms and Applications}, 
      author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
      year={2019},
      eprint={1812.05905},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{PI2020104222,
title = {Low-level autonomous control and tracking of quadrotor using reinforcement learning},
journal = {Control Engineering Practice},
volume = {95},
pages = {104222},
year = {2020},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2019.104222},
url = {https://www.sciencedirect.com/science/article/pii/S0967066119301923},
author = {Chen-Huan Pi and Kai-Chun Hu and Stone Cheng and I-Chen Wu},
keywords = {Reinforcement learning, Policy gradient, Quadrotor},
abstract = {This paper proposes a low-level quadrotor control algorithm using neural networks with model-free reinforcement learning, then explores the algorithm’s capabilities on quadrotor hover and tracking tasks. We provide a new point of view by examining the well-known policy gradient algorithm from reinforcement learning, then relaxing its requirements to improve training efficiency. Without requiring expert demonstrations, the improved algorithm is then applied to train a quadrotor controller with its output directly mapped to four actuators in a simulator, which is a technique used to control any linear or nonlinear system under unknown dynamic parameters and disturbances. We show two experimental tasks both in simulation and real-world quadrotors to verify our method and demonstrate performance: 1) hovering at a fixed position, and 2) tracking along a specific trajectory. The video of our experiments can be found at https://youtu.be/oEVcdiFPnMo.}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={Pmlr}
}

@misc{haarnoja2018soft,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{haarnoja2019learning,
      title={Learning to Walk via Deep Reinforcement Learning}, 
      author={Tuomas Haarnoja and Sehoon Ha and Aurick Zhou and Jie Tan and George Tucker and Sergey Levine},
      year={2019},
      eprint={1812.11103},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lu2020adaptive,
      title={Adaptive Online Planning for Continual Lifelong Learning}, 
      author={Kevin Lu and Igor Mordatch and Pieter Abbeel},
      year={2020},
      eprint={1912.01188},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NEURIPS2019_a34bacf8,
 author = {Ciosek, Kamil and Vuong, Quan and Loftin, Robert and Hofmann, Katja},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Better Exploration with Optimistic Actor Critic},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/a34bacf839b923770b2c360eefa26748-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{nature,
  title = {Champion-level drone racing using deep reinforcement learning},
  volume = {620},
  ISSN = {1476-4687},
  url = {http://dx.doi.org/10.1038/s41586-023-06419-4},
  DOI = {10.1038/s41586-023-06419-4},
  number = {7976},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author = {Kaufmann,  Elia and Bauersfeld,  Leonard and Loquercio,  Antonio and M\"{u}ller,  Matthias and Koltun,  Vladlen and Scaramuzza,  Davide},
  year = {2023},
  month = aug,
  pages = {982–987}
}